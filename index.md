---
layout: project_page
permalink: /

title: "Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models"
authors: Anonymous Author(s)
affiliations: Affiliation
paper: https://openreview.net/forum?id=EdVNB2kHv1
# video: 
# code: 
# data: 
---

<!-- <video width="100%" autoplay controls muted loop playsinline>
    <source src="" type="video/mp4">
</video> -->

---

<div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2>Abstract</h2>
        <div class="content has-text-justified">
A central challenge towards developing robots that can relate human language to their perception and actions is the scarcity of natural language annotations in diverse robot datasets. Moreover, robot policies that follow natural language instructions are typically trained on either templated language or expensive human-labeled instructions, hindering their scalability. To this end, we introduce <b>NILS</b>: <b>N</b>atural language <b>I</b>nstruction <b>L</b>abeling for <b>S</b>calability. NILS automatically labels uncurated, long-horizon robot data at scale in a zero-shot manner without any human intervention. NILS combines pre-trained vision-language foundation models in a sophisticated, carefully considered manner in order to detect objects in a scene, detect object-centric changes, segment tasks from large datasets of unlabelled interaction data and ultimately label behavior datasets. Evaluations on BridgeV2 and a kitchen play dataset show that NILS is able to autonomously annotate diverse robot demonstrations of unlabeled and unstructured datasets, while alleviating several shortcomings of crowdsourced human annotations.
        </div>
    </div>
</div>

---

<!-- > Note: This is an example of a Jekyll-based project website template: [Github link](https://github.com/shunzh/project_website).\
> The following content is generated by ChatGPT. The figure is manually added. -->

## Architecture
![MDT-V Overview](./static/image/lupus-example.png)
Overview of the proposed NILS framework for labeling long-horizon robot play sequences
in a zero-shot manner using an ensemble of pretrained expert models. NILS consists of three stages:
 - all relevant objects in the video are detected
 - object-centric changes are detected and collected
 - object change information is used to detect keystates and an LLM is prompted to generate a language label for the task

 <!-- <div class="video-container">
    <div class="video-header">
        <h3>View a Random Trajectory</h3>
        <div id="sample-button" class="replay">
            <img src="icons/replay.svg">
            <div>Sample</div>
        </div>
    </div>
    <div class="video-grid lang">
        <div class="method">Language Annotation</div>
        <div class="method">Initial</div>
        <div class="method">Final</div>
        <div class="task" id="annotation">closed the drawer</div>
        <div class="video"><img id="first_image" src="https://rail.eecs.berkeley.edu/datasets/bridge_release/raw/bridge_data_v2/datacol2_toykitchen7/drawer_pnp/10/2023-04-20_09-24-10/raw/traj_group0/traj29/images0/im_0.jpg"></div>
        <div class="video"><img id="last_image" src="https://rail.eecs.berkeley.edu/datasets/bridge_release/raw/bridge_data_v2/datacol2_toykitchen7/drawer_pnp/10/2023-04-20_09-24-10/raw/traj_group0/traj29/images0/im_37.jpg"></div>
    </div>
</div> -->

<!-- <script>
    // replay button animation
    new Image().src = 'icons/replay.svg' // preload

    const playButtons = document.querySelectorAll('.play-button');
    playButtons.forEach((button) => {
        button.addEventListener('click', () => {
            button.parentElement.parentElement.querySelectorAll('video').forEach((video) => {
                try {
                    video.fastSeek(0);
                } catch (error) {
                    video.currentTime = 0;
                }
                video.play();
            });
            const img = button.querySelector('img');
            img.src = 'icons/replay.svg';

            const text = button.querySelector('div');
            text.innerText = 'Replay';

            button.classList.remove('replay');
            void button.offsetWidth;
            button.classList.add('replay');
        });
    });

    // View Random Trajectory Widget
    var trajLinks;
    fetch("traj_links.csv")
        .then(response => response.text())
        .then(text => trajLinks = text.split(/\r\n|\n/))
        .then(sample);

    const prefix = "https://rail.eecs.berkeley.edu/datasets/bridge_release/";
    const firstImage = document.querySelector("#first_image");
    const lastImage = document.querySelector("#last_image");
    const annotation = document.querySelector("#annotation");
    function sample() {
        if (typeof trajLinks === 'undefined') return;

        firstImage.src = 'icons/dots.jpg';
        lastImage.src = 'icons/dots.jpg';
        annotation.innerText = 'loading...';

        const index = Math.floor(Math.random() * trajLinks.length);
        const links = trajLinks[index].split(",");

        firstImage.src = prefix + links[0];
        lastImage.src = prefix + links[1];
        annotation.innerText = links[2];
    }

    const sampleButton = document.querySelector('#sample-button');
    sampleButton.addEventListener('click', () => {
        if (firstImage.src.includes('dots.jpg') || lastImage.src.includes('dots.jpg')) return;

        sample();

        sampleButton.classList.remove('replay');
        void sampleButton.offsetWidth;
        sampleButton.classList.add('replay');
    });
</script> -->


<!-- ## 4 Columns

<div class="columns">
    <div class="column is-one-quarter">
        <h3>Heading 1</h3>
        <video autoplay controls>
            <source src="movie.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-quarter">
        <h3>Heading 2</h3>
        <video autoplay controls>
            <source src="movie.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-quarter">
        <h3>Heading 3</h3>
        <video autoplay controls>
            <source src="movie.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-quarter">
        <h3>Heading 4</h3>
        <video autoplay controls>
            <source src="movie.mp4" type="video/mp4">
        </video>
    </div>
</div> -->