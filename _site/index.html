<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models</title>

  <link href="https://fonts.googleapis.com/css?family=Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" />
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- MathJax -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
        displayMath: [ ['$$', '$$'], ['\\[', '\\]'] ],
        processEscapes: true, 
        tags: 'all',
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script
    type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models</h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous Author(s)
            </span>
          </div>
          

          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Affiliation
            </span>
          </div>
          

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              
              <span class="link-block">
                <a href="https://openreview.net/forum?id=EdVNB2kHv1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- Video Link. -->
              
              <!-- Code Link. -->
              
              <!-- Dataset Link. -->
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
<div class="container is-max-desktop">

<div class="content">
  <!-- <video width="100%" autoplay controls muted loop playsinline>
    <source src="" type="video/mp4">
</video> -->

<hr />

<div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2>Abstract</h2>
        <div class="content has-text-justified">
A central challenge towards developing robots that can relate human language to their perception and actions is the scarcity of natural language annotations in diverse robot datasets. Moreover, robot policies that follow natural language instructions are typically trained on either templated language or expensive human-labeled instructions, hindering their scalability. To this end, we introduce <b>NILS</b>: <b>N</b>atural language <b>I</b>nstruction <b>L</b>abeling for <b>S</b>calability. NILS automatically labels uncurated, long-horizon robot data at scale in a zero-shot manner without any human intervention. NILS combines pre-trained vision-language foundation models in a sophisticated, carefully considered manner in order to detect objects in a scene, detect object-centric changes, segment tasks from large datasets of unlabelled interaction data and ultimately label behavior datasets. Evaluations on BridgeV2 and a kitchen play dataset show that NILS is able to autonomously annotate diverse robot demonstrations of unlabeled and unstructured datasets, while alleviating several shortcomings of crowdsourced human annotations.
        </div>
    </div>
</div>

<hr />

<!-- > Note: This is an example of a Jekyll-based project website template: [Github link](https://github.com/shunzh/project_website).\
> The following content is generated by ChatGPT. The figure is manually added. -->

<h2 id="architecture">Architecture</h2>
<p><img src="./static/image/lupus-example.png" alt="MDT-V Overview" />
Overview of the proposed NILS framework for labeling long-horizon robot play sequences
in a zero-shot manner using an ensemble of pretrained expert models. NILS consists of three stages:</p>
<ul>
  <li>all relevant objects in the video are detected</li>
  <li>object-centric changes are detected and collected</li>
  <li>object change information is used to detect keystates and an LLM is prompted to generate a language label for the task</li>
</ul>

<h2>Examples</h2>

<div class="columns is-centered has-text-centered">
    <div class="column is-half" id="nils-video-container">
        <h4>Video</h4>
        <video id="nils-video" width="100%" muted="" playsinline="">
            <source src="" type="video/mp4" />
        </video>
    </div>
    <div class="column is-half">
        <h4>Last Keystate</h4>
        <canvas id="nils-keystate" width="100%" height="100%"></canvas>
    </div>
</div>
<div class="columns is-centered has-text-centered">
    <div class="column">
        <div class="buttons is-centered">
            <button id="nils-sample-button" class="button">Sample</button>
            <button id="nils-play-pause-button" class="button">Play/Pause</button>
            <!-- <button id="nils-prev-key-button" class="button">Previous Keystate</button>
            <button id="nils-next-key-button" class="button">Next Keystate</button> -->
        </div>
    </div>
</div>
<div class="columns is-centered has-text-centered">
    <div class="column">
        <h4>Generated Labels</h4>
        <div id="nils-labels-container"></div>
    </div>
</div>
<style>
    #nils-labels-container {
        display: flex;
        flex-direction: column;
        justify-content: space-evenly;
        align-items: center;
        gap: 1em;
    }
    #nils-keystate {
        width: 100%;
    }
</style>

<script>

const ANNOTATION_TYPES = [
    'all_gt_ks',
    'all',
    'enable_detection_ensemblingenable_object_state_filteringenable_scene_graph_denoisingenable_detection_refinmentenable_object_centric_relations',
    'enable_temporal_aggregationenable_detection_ensemblingenable_object_state_filteringenable_scene_graph_denoisingenable_detection_refinmentenable_object_centric_relationssimple_initial_object_detection',
    'enable_temporal_aggregationenable_object_state_filteringenable_scene_graph_denoisingenable_object_centric_relations',
    'gemini_pro',
    'gpt4v',
    'object_movement_gripper_close_scene_graph_object_state'
]

async function loadPaths() {
    const response = await fetch('./static/bridge_vis/paths.txt')
    const data = await response.text()
    const paths = data.split('\n')
    return paths
}

async function loadAnnotationsAndVideoLinkFromPath(path) {
    const responses = ANNOTATION_TYPES.map((type) => 
        fetch(`./static/bridge_vis/${path}/${type}.txt`)
            .then(response => {
                if (response.ok) {
                    return response.text()
                }
                return false
            })
            .catch(err => false)
    )

    const annotations_list = (await Promise.all(responses))
        .map((response, index) => response ? [ANNOTATION_TYPES[index], processAnnotation(response)] : null)
        .filter(item => item)
    const annotations = Object.fromEntries(annotations_list)

    const videoLink = `./static/bridge_vis/${path}/orig_conv.mp4`
    return { annotations, videoLink }
}

function processAnnotation(fileContent) {
    const lines = fileContent.split('\n')
    const parsedData = lines.map(line => {
        if (line === '') {
            return null
        }
        const [_, keystate, labels] = line.match(/Keystate: (\d+) - Annotation: (\[.*\])/)
        sanitized = labels // I hate this, why can't it be proper JSON, why are we using python print output uggh
            .replace(/\['/g, '["')
            .replace(/'\]/g, '"]')
            .replace(/', '/g, '", "')
            .replace(/", '/g, '", "')
            .replace(/', "/g, '", "')
        try {
            return {
                keystate: parseInt(keystate),
                labels: JSON.parse(sanitized)
            }
        } catch (e) {
            console.error(keystate, labels, sanitized, e)
            throw e
        }
    }).filter(item => item)
    return parsedData
}

function sampleFromPaths(paths) {
    const randomIndex = Math.floor(Math.random() * paths.length)
    return paths[randomIndex]
    // return paths[0]
}

const state = {
    annotations: null,
    selected_annotation_type: 'all_gt_ks',
    current_frame: 0,
    keystate: null,
    prev_keystate: null,
    fps: null,
    video_loaded: false
}

window.state = state

function resetState() {
    state.annotations = null
    state.current_frame = 0
    state.keystate = null
    state.prev_keystate = null
}

function updateUI() {
    if (!state.video_loaded) {
        return
    }

    const shouldUpdate = updateKeystate()
    if (shouldUpdate) {
        fillLabels()
        fillCanvas()
    }
}

function updateKeystate() {
    const annotations = state.annotations[state.selected_annotation_type]
    const keystates = annotations.map(label => label.keystate)
    const newKeystate = keystates.find(keystate => state.current_frame <= keystate) // array is short, so binary search not needed
    console.log(state.current_frame, newKeystate)
    if (newKeystate !== state.keystate) {
        state.prev_keystate = state.keystate
        state.keystate = newKeystate
        return true
    }
    return false
}

function fillLabels() {    
    const labels = state.annotations[state.selected_annotation_type].find(label => label.keystate === state.keystate).labels
    const labelsContainer = $('#nils-labels-container')
    labelsContainer.empty()
    labels.forEach(label => {
        const labelElement = $('<div></div>').text(label)
        labelsContainer.append(labelElement)
    })
}

function fillCanvas() {
    const video = $('#nils-video')[0]
    const canvas = $('#nils-keystate')[0]
    const ctx = canvas.getContext('2d')
    ctx.drawImage(video, 0, 0, video.videoWidth, video.videoHeight, 0, 0, video.videoWidth, video.videoHeight)
}

$(document).ready(async function() {
    const paths = await loadPaths()
    
    async function sample() {
        resetState()
        const path = sampleFromPaths(paths)
        const { annotations, videoLink } = await loadAnnotationsAndVideoLinkFromPath(path)
        state.annotations = annotations
        $('#nils-video source').attr('src', videoLink)
        $('#nils-video')[0].load()
        updateUI()
    }

    await sample()

    $("#nils-sample-button").click(async function() {
        await sample()
    })

    $("#nils-play-pause-button").click(function() {
        if (!state.video_loaded) {
            return
        }

        const video = $('#nils-video')[0]
        if (video.paused) {
            video.play()
        } else {
            video.pause()
        }
    })

    // $("#nils-prev-key-button").click(function() {
    //     if (!state.video_loaded) {
    //         return
    //     }
    //     const annotations = state.annotations[state.selected_annotation_type]
    //     const keystates = annotations.map(label => label.keystate)
    //     const currentIndex = keystates.indexOf(state.keystate)
    //     if (currentIndex > 0) {
    //         state.current_frame = keystates[currentIndex - 1]
    //         $('#nils-video')[0].currentTime = state.current_frame / state.fps
    //         updateUI()
    //     }
    // })

    // $("#nils-next-key-button").click(function() {
    //     if (!state.video_loaded) {
    //         return
    //     }
    //     const annotations = state.annotations[state.selected_annotation_type]
    //     const keystates = annotations.map(label => label.keystate)
    //     const currentIndex = keystates.indexOf(state.keystate)
    //     if (currentIndex < keystates.length) {
    //         state.current_frame = keystates[currentIndex]
    //         $('#nils-video')[0].currentTime = state.current_frame / state.fps
    //         updateUI()
    //     }
    // })

    $('#nils-video').on('timeupdate', function() {
        state.current_frame = Math.floor(this.currentTime * state.fps)
        updateUI()
    })

    $('#nils-video').on('loadeddata', function() {
        const video = $('#nils-video')[0]
        const canvas = $('#nils-keystate')[0]
        const videoWidth = video.videoWidth
        const videoHeight = video.videoHeight
        canvas.width = videoWidth
        canvas.height = videoHeight

        state.fps = state.annotations[state.selected_annotation_type].slice(-1)[0].keystate / video.duration
        state.video_loaded = true
    });
})


</script>


</div>

</div>
</section>

<footer class="footer">
</footer>

</body>
</html>
