<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models</title>

  <link href="https://fonts.googleapis.com/css?family=Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" />
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- MathJax -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
        displayMath: [ ['$$', '$$'], ['\\[', '\\]'] ],
        processEscapes: true, 
        tags: 'all',
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script
    type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models</h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous Author(s)
            </span>
          </div>
          

          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Affiliation
            </span>
          </div>
          

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              
              <!-- Video Link. -->
              
              <!-- Code Link. -->
              
              <!-- Dataset Link. -->
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
<div class="container is-max-desktop">

<div class="content">
  <!-- <video width="100%" autoplay controls muted loop playsinline>
    <source src="./static/videos/mdt-v5-encoded.mp4" type="video/mp4">
</video> -->

<hr />

<div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2>Abstract</h2>
        <div class="content has-text-justified">
A central challenge towards developing robots that can relate human language to their perception and actions is the scarcity of natural language annotations in diverse robot datasets. Moreover, robot policies that follow natural language instructions are typically trained on either templated language or expensive human-labeled instructions, hindering their scalability. To this end, we introduce a novel approach to automatically label uncurated, long-horizon robot teleoperation data at scale in a zero-shot manner without any human intervention. We utilize a combination of pre-trained vision-language foundation models to detect objects in a scene, detect object-centric changes, segment tasks from large datasets of unlabelled interaction data and then train language-conditioned policies on the relabeled datasets. Our experiments on dataset show that our method enables annotating diverse robot demonstrations and thus scaling training language-conditioned policies on unlabeled and unstructured datasets that match ones trained with oracle human annotations.
        </div>
    </div>
</div>

<hr />

<!-- > Note: This is an example of a Jekyll-based project website template: [Github link](https://github.com/shunzh/project_website).\
> The following content is generated by ChatGPT. The figure is manually added. -->

<!-- ## Model Architecture
![MDT-V Overview](./static/image/mdt-v-figure.png)
**Left**: Overview of the proposed multimodal Transformer-Encoder-Decoder Diffusion Policy used in MDT.
**Right**: Specialized Diffusion Transformer Block for the Denoising of the Action Sequence.

MDT learns a goal-conditioned latent
state representation from multiple image observations and multimodal goals. The camera images are either processed with
frozen Voltron Encoders and a Perceiver or using ResNets. The separate GPT denoising module iteratively denoises an action
sequence of 10 steps with a Transformer Decoder with causal Attention. It consists of several Denoising Blocks, as visualized
on the right side. These blocks process noisy action tokens with self-attention and fuse the conditioning information from the
latent state representation via cross-attention. MDT applies adaLN conditioning to condition the blocks on the current
noise level. In addition, it aligns the latent representation tokens of the same state with different goal specifications using
self-supervised contrastive learning. The latent representation tokens are also used as a context input for the masked Image
Decoder module to reconstruct masked-out patches from future images.

### Masked Generative Foresight
<div class="column is-half is-pulled-right p-0">
    <img src="./static/image/mgf.png" alt="Masked Generative Foresight"/>
</div>
The Masked Generative Foresight Auxiliary Task
enhances the MDT model. It starts by encoding the current
observation and goal using the MDT Encoder. The resulting
latent state representations then serve as conditional inputs
for the Future Image-Decoder. This decoder receives encoded
patches of future camera images along with mask tokens. Its
task is to reconstruct the occluded patches in future frames.


### Contrastive Latent Alignment

Contrastive Latent Alignment auxiliary objective aligns
the MDT(-V) embeddings across
different goal modalities. These embeddings include the goal
as well as the current state information, allowing the CLA
objective to consider the task dynamics. 
Every training sample that is paired with a multimodal
goals specification is projected to latent vectors
for images and language goals respectively.
Contrastive Latent Alignment is achieved by using the InfoNCE loss with cosine similarity between the image and language projection.


## State-of-the-art on CALVIN ABCDâ†’D
MDT-V sets a
new record in the CALVIN challenge, extending the average
rollout length to **4.51** which is a **10% absolute improvement**
over RoboFlamingo. MDT also surpasses all other tested
methods. Notably, MDT achieves this while having less than
10% of trainable parameters and not requiring pretraining
on large-scale datasets.

<div class="columns is-centered">
    <div class="column is-two-thirds">
        <img src="./static/image/calvin-abcd.png" alt="CALVIN ABCD->D"/>
    </div>
</div>
<div class="columns is-mobile is-multiline is-centered">
    <div class="column is-half-mobile is-one-third-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/mdt_02.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-third-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/5_seq_mdt_rollout_text_3.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-third-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/5-seq_mdt_rollout_text_4.mp4" type="video/mp4">
        </video>
    </div>
</div>
<div class="columns is-mobile is-multiline is-centered">
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/long_horizon_sequence_0_30000.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/long_horizon_sequence_1_30000.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/long_horizon_sequence_2_30000.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/long_horizon_sequence_3_30000.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/long_horizon_sequence_4_30000.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/long_horizon_sequence_5_30000.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/long_horizon_sequence_6_30000.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/long_horizon_sequence_7_30000.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/long_horizon_sequence_8_30000.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/long_horizon_sequence_9_30000.mp4" type="video/mp4">
        </video>
    </div>
</div>

## LIBERO with less than 2% Language Annotations

In the LIBERO task suites, MDT proves to be effective with sparsely labeled
data, outperforming the Oracle-BC baseline, which relies on
fully labeled demonstrations. MDT not only outperforms the
fully language-labeled Transformer Baseline in three out of
four challenges but also significantly surpasses the U-Net-
based Distill-D policy in all tests by a wide margin, even
without auxiliary objectives. The performance of MDT on the
LIBERO-90 suite demonstrates that both objectives and our
policy learn best from a large dataset. The proposed auxiliary
objectives further improve the average performance of MDT
by 8.5% averaged over all 5 task suites.

<div class="columns is-mobile is-multiline is-centered">
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p0_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p1_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p2_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p3_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p4_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p5_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p6_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p7_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p8_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p9_videos/video.mp4" type="video/mp4">
        </video>
    </div>
</div>

## Real Robot Experiments
Real world play dataset encompasses
around 4.5 hours of interactive **play data** with **20 different
tasks** for the policies to learn. Play demonstrations last
from around 30 seconds to more than 450 seconds and contain
between 5 and 20 tasks. The dataset is **partially labeled** by
randomly identifying some tasks in the demonstrations and
annotating the respective interval, yielding a total of 360 labels (~18 labels per task)
or approximately 20% of the dataset.

### Sample Demonstration from the Real Robot Dataset
<video width="100%" autoplay controls muted loop playsinline>
    <source src="./static/videos/demonstration.mp4" type="video/mp4">
</video>
### Evaluation Videos
#### Multi-task
<div class="columns is-centered is-mobile">
    <div class="column is-half">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/m1.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/m2.mp4" type="video/mp4">
        </video>
    </div>    
</div>
#### Single-task
<div class="columns is-centered is-multiline is-mobile">
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s1.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s2.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s3.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s4.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s5.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s6.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s7.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s8.mp4" type="video/mp4">
        </video>
    </div>
</div>

## Citation
```
@inproceedings{
    reuss2024multimodal,
    title={Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals},
    author={Moritz Reuss and {\"O}mer Erdin{\c{c}} Ya{\u{g}}murlu and Fabian Wenzel and Rudolf Lioutikov},
    booktitle={Robotics: Science and Systems},
    year={2024}
    }
``` -->

</div>

</div>
</section>

<!-- <footer class="footer">
</footer> -->

</body>
</html>
